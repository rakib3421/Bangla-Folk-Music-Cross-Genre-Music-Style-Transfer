{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5ae289",
   "metadata": {},
   "source": [
    "# Cross-Genre Music Style Transfer: Bangla Folk → Rock/Jazz\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains the complete implementation of a deep learning system for cross-genre music style transfer, specifically designed to transform **Bengali Folk music** into **Rock** and **Jazz** styles while preserving vocal characteristics, rhythmic patterns, and musical structure.\n",
    "\n",
    "### Key Features\n",
    "- **Multi-Genre Support**: Bengali Folk → Rock/Jazz transformation\n",
    "- **Vocal Preservation**: Maintains vocal characteristics during style transfer\n",
    "- **Rhythmic Awareness**: Preserves and adapts rhythmic patterns\n",
    "- **Musical Structure**: Respects song structure and harmonic progressions\n",
    "- **Real-time Processing**: Optimized models for live performance\n",
    "- **Interactive Control**: User-adjustable style intensity and blending\n",
    "\n",
    "### System Architecture\n",
    "```\n",
    "Input Audio (Bengali Folk) → Audio Preprocessing → Style Transfer Engine → Quality Enhancement → Output Audio (Rock/Jazz Style)\n",
    "```\n",
    "\n",
    "This notebook includes all core components: audio processing, neural network models, training pipeline, evaluation framework, and interactive controls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb053ec5",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "This section installs required packages and imports necessary libraries for the music style transfer system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: Run this cell first to install dependencies\n",
    "!pip install librosa soundfile pydub numpy scipy pandas essentia madmom mir_eval spleeter matplotlib seaborn plotly torch torchaudio tqdm joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d71052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for audio processing and machine learning\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing libraries\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Scientific computing and data handling\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "# Deep learning framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "# Progress tracking and utilities\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Librosa version: {librosa.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89689bb9",
   "metadata": {},
   "source": [
    "## 2. Audio Preprocessing Module\n",
    "\n",
    "This section contains the audio preprocessing utilities for standardizing audio files, handling format conversion, normalization, and segmentation for the style transfer pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Audio preprocessing utilities for cross-genre music style transfer.\n",
    "Handles format standardization, normalization, and segmentation.\n",
    "\"\"\"\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    \"\"\"\n",
    "    Audio preprocessing class for standardizing audio files for style transfer.\n",
    "\n",
    "    This class provides methods for:\n",
    "    - Loading and saving audio files\n",
    "    - Format standardization (44.1kHz, 16-bit WAV)\n",
    "    - Audio normalization\n",
    "    - File segmentation for long recordings\n",
    "    - Dataset processing and analysis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_sr=44100, target_format='wav', segment_duration=30):\n",
    "        \"\"\"\n",
    "        Initialize audio preprocessor with target specifications.\n",
    "\n",
    "        Args:\n",
    "            target_sr (int): Target sample rate in Hz (default: 44100)\n",
    "            target_format (str): Target audio format (default: 'wav')\n",
    "            segment_duration (int): Duration for segmentation in seconds (default: 30)\n",
    "        \"\"\"\n",
    "        self.target_sr = target_sr\n",
    "        self.target_format = target_format\n",
    "        self.segment_duration = segment_duration\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"\n",
    "        Load audio file for processing.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to audio file\n",
    "\n",
    "        Returns:\n",
    "            tuple: (audio_data, sample_rate) as numpy array and int\n",
    "        \"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=self.target_sr, mono=True)\n",
    "            # Normalize audio to prevent clipping\n",
    "            y = librosa.util.normalize(y)\n",
    "            return y, sr\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio {file_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_audio(self, audio, output_path, sr):\n",
    "        \"\"\"\n",
    "        Save audio data to file.\n",
    "\n",
    "        Args:\n",
    "            audio: Audio data as numpy array\n",
    "            output_path: Output file path\n",
    "            sr: Sample rate\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sf.write(output_path, audio, sr, subtype='PCM_16')\n",
    "            print(f\"Successfully saved audio to: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving audio to '{output_path}': {str(e)}\")\n",
    "            # Try alternative method without subtype\n",
    "            try:\n",
    "                sf.write(output_path, audio, sr)\n",
    "                print(f\"Successfully saved audio using fallback method: {output_path}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Fallback save also failed: {str(e2)}\")\n",
    "                raise e\n",
    "\n",
    "    def standardize_audio_file(self, input_path, output_path):\n",
    "        \"\"\"\n",
    "        Convert audio file to standard format with normalization.\n",
    "\n",
    "        Args:\n",
    "            input_path (str): Path to input audio file\n",
    "            output_path (str): Path to output standardized file\n",
    "\n",
    "        Returns:\n",
    "            tuple: (success, duration) where success is bool and duration is float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio file\n",
    "            y, sr = librosa.load(input_path, sr=self.target_sr, mono=True)\n",
    "\n",
    "            # Normalize audio to prevent clipping\n",
    "            y = librosa.util.normalize(y)\n",
    "\n",
    "            # Save as 16-bit WAV\n",
    "            sf.write(output_path, y, self.target_sr, subtype='PCM_16')\n",
    "\n",
    "            return True, len(y) / self.target_sr  # Return success and duration\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_path}: {str(e)}\")\n",
    "            return False, 0\n",
    "\n",
    "    def segment_audio(self, audio_path, output_dir, min_duration=10):\n",
    "        \"\"\"\n",
    "        Segment long audio files into manageable chunks.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to audio file\n",
    "            output_dir (str): Directory to save segments\n",
    "            min_duration (int): Minimum duration for a segment in seconds\n",
    "\n",
    "        Returns:\n",
    "            list: List of paths to created segments\n",
    "        \"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=self.target_sr)\n",
    "            duration = len(y) / sr\n",
    "\n",
    "            # If file is shorter than segment duration, copy as is\n",
    "            if duration <= self.segment_duration:\n",
    "                base_name = Path(audio_path).stem\n",
    "                output_path = os.path.join(output_dir, f\"{base_name}_segment_01.wav\")\n",
    "                sf.write(output_path, y, sr, subtype='PCM_16')\n",
    "                return [output_path]\n",
    "\n",
    "            # Create segments\n",
    "            segment_samples = self.segment_duration * sr\n",
    "            segments = []\n",
    "\n",
    "            for i, start in enumerate(range(0, len(y), segment_samples)):\n",
    "                end = min(start + segment_samples, len(y))\n",
    "                segment = y[start:end]\n",
    "\n",
    "                # Skip segments that are too short\n",
    "                if len(segment) / sr < min_duration:\n",
    "                    continue\n",
    "\n",
    "                base_name = Path(audio_path).stem\n",
    "                output_path = os.path.join(output_dir, f\"{base_name}_segment_{i+1:02d}.wav\")\n",
    "                sf.write(output_path, segment, sr, subtype='PCM_16')\n",
    "                segments.append(output_path)\n",
    "\n",
    "            return segments\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error segmenting {audio_path}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def process_genre_dataset(self, input_dir, output_dir, segment=True):\n",
    "        \"\"\"\n",
    "        Process all audio files in a genre directory.\n",
    "\n",
    "        Args:\n",
    "            input_dir (str): Input directory containing audio files\n",
    "            output_dir (str): Output directory for processed files\n",
    "            segment (bool): Whether to segment long files\n",
    "\n",
    "        Returns:\n",
    "            tuple: (processed_files, stats) where processed_files is list of paths and stats is dict\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Get all audio files\n",
    "        audio_extensions = {'.mp3', '.wav', '.flac', '.m4a', '.aac'}\n",
    "        audio_files = []\n",
    "\n",
    "        for ext in audio_extensions:\n",
    "            audio_files.extend(Path(input_dir).glob(f\"*{ext}\"))\n",
    "\n",
    "        print(f\"Found {len(audio_files)} audio files in {input_dir}\")\n",
    "\n",
    "        processed_files = []\n",
    "        stats = {\n",
    "            'total_files': len(audio_files),\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'total_duration': 0,\n",
    "            'segments_created': 0\n",
    "        }\n",
    "\n",
    "        for audio_file in tqdm(audio_files, desc=\"Processing audio files\"):\n",
    "            if segment:\n",
    "                # Create segments\n",
    "                segments = self.segment_audio(str(audio_file), output_dir)\n",
    "                if segments:\n",
    "                    processed_files.extend(segments)\n",
    "                    stats['segments_created'] += len(segments)\n",
    "                    stats['successful'] += 1\n",
    "                else:\n",
    "                    stats['failed'] += 1\n",
    "            else:\n",
    "                # Direct conversion\n",
    "                output_name = f\"{audio_file.stem}.wav\"\n",
    "                output_path = os.path.join(output_dir, output_name)\n",
    "\n",
    "                success, duration = self.standardize_audio_file(str(audio_file), output_path)\n",
    "                if success:\n",
    "                    processed_files.append(output_path)\n",
    "                    stats['successful'] += 1\n",
    "                    stats['total_duration'] += duration\n",
    "                else:\n",
    "                    stats['failed'] += 1\n",
    "\n",
    "        return processed_files, stats\n",
    "\n",
    "    def get_audio_info(self, file_path):\n",
    "        \"\"\"\n",
    "        Get basic information about an audio file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to audio file\n",
    "\n",
    "        Returns:\n",
    "            dict: Audio file information including sample rate, duration, etc.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            duration = len(y) / sr\n",
    "\n",
    "            return {\n",
    "                'file_path': file_path,\n",
    "                'sample_rate': sr,\n",
    "                'duration': duration,\n",
    "                'num_samples': len(y),\n",
    "                'file_size': os.path.getsize(file_path)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'file_path': file_path,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "def create_dataset_info(data_dir):\n",
    "    \"\"\"\n",
    "    Create a comprehensive overview of the dataset.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to data directory\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset information\n",
    "    \"\"\"\n",
    "    preprocessor = AudioPreprocessor()\n",
    "    dataset_info = []\n",
    "\n",
    "    genres = ['Bangla Folk', 'Jazz', 'Rock']\n",
    "\n",
    "    for genre in genres:\n",
    "        genre_dir = os.path.join(data_dir, genre)\n",
    "        if not os.path.exists(genre_dir):\n",
    "            continue\n",
    "\n",
    "        audio_extensions = {'.mp3', '.wav', '.flac', '.m4a', '.aac'}\n",
    "        audio_files = []\n",
    "\n",
    "        for ext in audio_extensions:\n",
    "            audio_files.extend(Path(genre_dir).glob(f\"*{ext}\"))\n",
    "\n",
    "        print(f\"Analyzing {len(audio_files)} files in {genre}...\")\n",
    "\n",
    "        for audio_file in tqdm(audio_files[:10], desc=f\"Sampling {genre} files\"):  # Sample first 10 files\n",
    "            info = preprocessor.get_audio_info(str(audio_file))\n",
    "            info['genre'] = genre\n",
    "            dataset_info.append(info)\n",
    "\n",
    "    return pd.DataFrame(dataset_info)\n",
    "\n",
    "# Initialize the audio preprocessor\n",
    "audio_preprocessor = AudioPreprocessor()\n",
    "print(\"Audio preprocessing module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc9cd2",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Module\n",
    "\n",
    "This section contains the feature extraction utilities for analyzing audio characteristics including mel-spectrograms, chromagrams, rhythm features, and timbral features used in the style transfer process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815241a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature extraction module for cross-genre music style transfer.\n",
    "Extracts mel-spectrograms, chromagrams, rhythm features, and timbral features.\n",
    "\"\"\"\n",
    "\n",
    "class AudioFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature extractor for audio analysis in style transfer.\n",
    "\n",
    "    This class provides methods to extract various audio features:\n",
    "    - Mel-spectrograms: Frequency representation for neural networks\n",
    "    - Chromagrams: Harmonic content analysis\n",
    "    - Rhythm features: Tempo, beat tracking, tempograms\n",
    "    - Timbral features: MFCCs, spectral characteristics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sr=44100, hop_length=512, n_fft=2048):\n",
    "        \"\"\"\n",
    "        Initialize feature extractor with audio processing parameters.\n",
    "\n",
    "        Args:\n",
    "            sr (int): Sample rate in Hz\n",
    "            hop_length (int): Hop length for STFT analysis\n",
    "            n_fft (int): FFT window size\n",
    "        \"\"\"\n",
    "        self.sr = sr\n",
    "        self.hop_length = hop_length\n",
    "        self.n_fft = n_fft\n",
    "\n",
    "    def extract_mel_spectrogram(self, y, n_mels=128):\n",
    "        \"\"\"\n",
    "        Extract mel-spectrogram features for neural network input.\n",
    "\n",
    "        Args:\n",
    "            y (np.array): Audio time series\n",
    "            n_mels (int): Number of mel frequency bands\n",
    "\n",
    "        Returns:\n",
    "            np.array: Mel-spectrogram in dB scale\n",
    "        \"\"\"\n",
    "        # Compute mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=self.sr, n_mels=n_mels,\n",
    "            hop_length=self.hop_length, n_fft=self.n_fft\n",
    "        )\n",
    "        # Convert to dB scale for better neural network training\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec_db\n",
    "\n",
    "    def extract_chromagram(self, y, n_chroma=12):\n",
    "        \"\"\"\n",
    "        Extract chromagram for harmonic content analysis.\n",
    "\n",
    "        Chromagrams represent the 12-bin chroma vector for each time frame,\n",
    "        useful for analyzing harmonic progression and key changes.\n",
    "\n",
    "        Args:\n",
    "            y (np.array): Audio time series\n",
    "            n_chroma (int): Number of chroma bins (default: 12 for semitones)\n",
    "\n",
    "        Returns:\n",
    "            np.array: Chromagram matrix\n",
    "        \"\"\"\n",
    "        chroma = librosa.feature.chroma_stft(\n",
    "            y=y, sr=self.sr, hop_length=self.hop_length, n_fft=self.n_fft\n",
    "        )\n",
    "        return chroma\n",
    "\n",
    "    def extract_rhythm_features(self, y):\n",
    "        \"\"\"\n",
    "        Extract rhythm-related features including tempo and beat tracking.\n",
    "\n",
    "        Args:\n",
    "            y (np.array): Audio time series\n",
    "\n",
    "        Returns:\n",
    "            dict: Rhythm features including tempo, beats, tempogram, and beat-synchronous chroma\n",
    "        \"\"\"\n",
    "        # Tempo estimation using beat tracking\n",
    "        tempo, beats = librosa.beat.beat_track(y=y, sr=self.sr, hop_length=self.hop_length)\n",
    "\n",
    "        # Rhythm patterns (tempogram) - represents rhythmic content over time\n",
    "        tempogram = librosa.feature.tempogram(\n",
    "            y=y, sr=self.sr, hop_length=self.hop_length\n",
    "        )\n",
    "\n",
    "        # Beat synchronous chroma - chroma features aligned to beats\n",
    "        chroma_sync = librosa.feature.chroma_stft(\n",
    "            y=y, sr=self.sr, hop_length=self.hop_length\n",
    "        )\n",
    "        if len(beats) > 0:\n",
    "            chroma_sync = librosa.util.sync(chroma_sync, beats)\n",
    "\n",
    "        return {\n",
    "            'tempo': tempo,\n",
    "            'beats': beats,\n",
    "            'tempogram': tempogram,\n",
    "            'beat_chroma': chroma_sync,\n",
    "            'beat_count': len(beats)\n",
    "        }\n",
    "\n",
    "    def extract_timbral_features(self, y):\n",
    "        \"\"\"\n",
    "        Extract timbral features that characterize the sound quality and texture.\n",
    "\n",
    "        Args:\n",
    "            y (np.array): Audio time series\n",
    "\n",
    "        Returns:\n",
    "            dict: Timbral features including MFCCs and spectral descriptors\n",
    "        \"\"\"\n",
    "        # MFCC features - most important timbral features\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            y=y, sr=self.sr, n_mfcc=13,\n",
    "            hop_length=self.hop_length, n_fft=self.n_fft\n",
    "        )\n",
    "\n",
    "        # Spectral features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(\n",
    "            y=y, sr=self.sr, hop_length=self.hop_length\n",
    "        )\n",
    "\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(\n",
    "            y=y, sr=self.sr, hop_length=self.hop_length\n",
    "        )\n",
    "\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(\n",
    "            y=y, sr=self.sr, hop_length=self.hop_length\n",
    "        )\n",
    "\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(\n",
    "            y, hop_length=self.hop_length\n",
    "        )\n",
    "\n",
    "        # RMS energy\n",
    "        rms = librosa.feature.rms(y=y, hop_length=self.hop_length)\n",
    "\n",
    "        return {\n",
    "            'mfcc': mfccs,\n",
    "            'spectral_centroid': spectral_centroids,\n",
    "            'spectral_rolloff': spectral_rolloff,\n",
    "            'spectral_bandwidth': spectral_bandwidth,\n",
    "            'zero_crossing_rate': zero_crossing_rate,\n",
    "            'rms': rms\n",
    "        }\n",
    "\n",
    "    def extract_all_features(self, audio_file):\n",
    "        \"\"\"\n",
    "        Extract complete feature set from an audio file.\n",
    "\n",
    "        Args:\n",
    "            audio_file (str): Path to audio file\n",
    "\n",
    "        Returns:\n",
    "            dict: Complete feature set or None if extraction fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(audio_file, sr=self.sr)\n",
    "\n",
    "            # Extract all feature types\n",
    "            features = {}\n",
    "\n",
    "            # Mel-spectrogram (primary input for neural networks)\n",
    "            features['mel_spectrogram'] = self.extract_mel_spectrogram(y)\n",
    "\n",
    "            # Chromagram (harmonic content)\n",
    "            features['chromagram'] = self.extract_chromagram(y)\n",
    "\n",
    "            # Rhythm features\n",
    "            rhythm_features = self.extract_rhythm_features(y)\n",
    "            features.update(rhythm_features)\n",
    "\n",
    "            # Timbral features\n",
    "            timbral_features = self.extract_timbral_features(y)\n",
    "            features.update(timbral_features)\n",
    "\n",
    "            # Basic audio info\n",
    "            features['duration'] = len(y) / sr\n",
    "            features['audio_file'] = audio_file\n",
    "\n",
    "            return features\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features from {audio_file}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def compute_feature_statistics(self, features):\n",
    "        \"\"\"\n",
    "        Compute statistical summaries of time-varying features.\n",
    "\n",
    "        Args:\n",
    "            features (dict): Feature dictionary from extract_all_features\n",
    "\n",
    "        Returns:\n",
    "            dict: Statistical summaries (mean, std, median) for each feature\n",
    "        \"\"\"\n",
    "        stats_features = {}\n",
    "\n",
    "        # Features to compute statistics for\n",
    "        time_varying_features = [\n",
    "            'mel_spectrogram', 'chromagram', 'tempogram',\n",
    "            'mfcc', 'spectral_centroid', 'spectral_rolloff',\n",
    "            'spectral_bandwidth', 'zero_crossing_rate', 'rms'\n",
    "        ]\n",
    "\n",
    "        for feature_name in time_varying_features:\n",
    "            if feature_name in features:\n",
    "                feature_data = features[feature_name]\n",
    "\n",
    "                if feature_data.ndim > 1:\n",
    "                    # For 2D features, compute stats across time axis\n",
    "                    stats_features[f'{feature_name}_mean'] = np.mean(feature_data, axis=1)\n",
    "                    stats_features[f'{feature_name}_std'] = np.std(feature_data, axis=1)\n",
    "                    stats_features[f'{feature_name}_median'] = np.median(feature_data, axis=1)\n",
    "                else:\n",
    "                    # For 1D features\n",
    "                    stats_features[f'{feature_name}_mean'] = np.mean(feature_data)\n",
    "                    stats_features[f'{feature_name}_std'] = np.std(feature_data)\n",
    "                    stats_features[f'{feature_name}_median'] = np.median(feature_data)\n",
    "\n",
    "        # Add scalar features directly\n",
    "        scalar_features = ['tempo', 'beat_count', 'duration']\n",
    "        for feature_name in scalar_features:\n",
    "            if feature_name in features:\n",
    "                stats_features[feature_name] = features[feature_name]\n",
    "\n",
    "        return stats_features\n",
    "\n",
    "class GenreAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzer for comparing characteristics across different music genres.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with feature extractor.\"\"\"\n",
    "        self.feature_extractor = AudioFeatureExtractor()\n",
    "\n",
    "    def analyze_genre_characteristics(self, audio_files, genre_name):\n",
    "        \"\"\"\n",
    "        Analyze characteristic features of a specific genre.\n",
    "\n",
    "        Args:\n",
    "            audio_files (list): List of audio file paths\n",
    "            genre_name (str): Name of the genre\n",
    "\n",
    "        Returns:\n",
    "            dict: Genre analysis results with statistics and raw data\n",
    "        \"\"\"\n",
    "        print(f\"Analyzing {len(audio_files)} {genre_name} files...\")\n",
    "\n",
    "        all_features = []\n",
    "\n",
    "        # Extract features from a subset of files for analysis\n",
    "        sample_size = min(20, len(audio_files))\n",
    "        sample_files = np.random.choice(audio_files, sample_size, replace=False)\n",
    "\n",
    "        for audio_file in sample_files:\n",
    "            features = self.feature_extractor.extract_all_features(audio_file)\n",
    "            if features:\n",
    "                stats = self.feature_extractor.compute_feature_statistics(features)\n",
    "                stats['genre'] = genre_name\n",
    "                stats['file'] = audio_file\n",
    "                all_features.append(stats)\n",
    "\n",
    "        if not all_features:\n",
    "            return None\n",
    "\n",
    "        # Convert to DataFrame for analysis\n",
    "        df = pd.DataFrame(all_features)\n",
    "\n",
    "        # Compute genre statistics\n",
    "        genre_stats = {}\n",
    "\n",
    "        # Tempo analysis\n",
    "        if 'tempo' in df.columns:\n",
    "            genre_stats['tempo'] = {\n",
    "                'mean': df['tempo'].mean(),\n",
    "                'std': df['tempo'].std(),\n",
    "                'median': df['tempo'].median(),\n",
    "                'range': [df['tempo'].min(), df['tempo'].max()]\n",
    "            }\n",
    "\n",
    "        # Spectral characteristics\n",
    "        spectral_features = [col for col in df.columns if 'spectral' in col]\n",
    "        for feature in spectral_features:\n",
    "            if feature in df.columns:\n",
    "                genre_stats[feature] = {\n",
    "                    'mean': df[feature].mean() if df[feature].dtype in ['float64', 'int64'] else 'N/A',\n",
    "                    'std': df[feature].std() if df[feature].dtype in ['float64', 'int64'] else 'N/A'\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            'genre': genre_name,\n",
    "            'sample_count': len(all_features),\n",
    "            'statistics': genre_stats,\n",
    "            'raw_data': df\n",
    "        }\n",
    "\n",
    "def analyze_dataset_characteristics(data_dir):\n",
    "    \"\"\"\n",
    "    Analyze characteristics of all genres in the dataset.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to data directory\n",
    "\n",
    "    Returns:\n",
    "        dict: Complete dataset analysis for all genres\n",
    "    \"\"\"\n",
    "    analyzer = GenreAnalyzer()\n",
    "    results = {}\n",
    "\n",
    "    genres = ['Bangla Folk', 'Jazz', 'Rock']\n",
    "\n",
    "    for genre in genres:\n",
    "        genre_dir = os.path.join(data_dir, genre)\n",
    "        if not os.path.exists(genre_dir):\n",
    "            continue\n",
    "\n",
    "        # Get audio files\n",
    "        audio_extensions = {'.mp3', '.wav', '.flac', '.m4a', '.aac'}\n",
    "        audio_files = []\n",
    "\n",
    "        for ext in audio_extensions:\n",
    "            audio_files.extend(str(f) for f in Path(genre_dir).glob(f\"*{ext}\"))\n",
    "\n",
    "        if audio_files:\n",
    "            analysis = analyzer.analyze_genre_characteristics(audio_files, genre)\n",
    "            if analysis:\n",
    "                results[genre] = analysis\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = AudioFeatureExtractor()\n",
    "genre_analyzer = GenreAnalyzer()\n",
    "\n",
    "print(\"Feature extraction module loaded successfully!\")\n",
    "print(\"Available methods:\")\n",
    "print(\"- extract_mel_spectrogram(): Mel-spectrogram extraction\")\n",
    "print(\"- extract_chromagram(): Harmonic content analysis\")\n",
    "print(\"- extract_rhythm_features(): Tempo and beat tracking\")\n",
    "print(\"- extract_timbral_features(): Sound texture analysis\")\n",
    "print(\"- extract_all_features(): Complete feature extraction\")\n",
    "print(\"- analyze_genre_characteristics(): Genre comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd213a0a",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture Module\n",
    "\n",
    "This section contains the deep learning models for style transfer, including CycleGAN and StarGAN-VC architectures designed for mel-spectrogram transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CycleGAN Architecture for Cross-Genre Music Style Transfer\n",
    "Implements generator and discriminator networks for multi-domain audio style transfer.\n",
    "\"\"\"\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with instance normalization for generator bottleneck.\n",
    "\n",
    "    This block implements the skip connection: output = input + F(input)\n",
    "    where F is a sequence of convolution, normalization, and activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, kernel_size: int = 3, padding: int = 1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding, bias=False)\n",
    "        self.norm1 = nn.InstanceNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding, bias=False)\n",
    "        self.norm2 = nn.InstanceNorm2d(channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        out = F.relu(self.norm1(self.conv1(x)))\n",
    "        out = self.norm2(self.conv2(out))\n",
    "        return residual + out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator network for mel-spectrogram style transfer.\n",
    "\n",
    "    Architecture: Encoder -> Bottleneck (Residual blocks) -> Decoder\n",
    "\n",
    "    The generator transforms mel-spectrograms from one musical style to another\n",
    "    while preserving content and musical structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 1,\n",
    "        output_channels: int = 1,\n",
    "        base_channels: int = 64,\n",
    "        n_residual_blocks: int = 9,\n",
    "        input_height: int = 128,\n",
    "        input_width: int = 256\n",
    "    ):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "\n",
    "        # Encoder: Downsampling layers to extract features\n",
    "        encoder = [\n",
    "            # Initial convolution to extract low-level features\n",
    "            nn.Conv2d(input_channels, base_channels, kernel_size=7, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(base_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Progressive downsampling to capture hierarchical features\n",
    "        in_channels = base_channels\n",
    "        for i in range(2):  # 2 downsampling layers\n",
    "            out_channels = in_channels * 2\n",
    "            encoder += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder)\n",
    "\n",
    "        # Bottleneck: Residual blocks for style transformation\n",
    "        bottleneck = []\n",
    "        for _ in range(n_residual_blocks):\n",
    "            bottleneck.append(ResidualBlock(in_channels))\n",
    "\n",
    "        self.bottleneck = nn.Sequential(*bottleneck)\n",
    "\n",
    "        # Decoder: Upsampling layers to reconstruct output\n",
    "        decoder = []\n",
    "        for i in range(2):  # 2 upsampling layers\n",
    "            out_channels = in_channels // 2\n",
    "            decoder += [\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2,\n",
    "                                 padding=1, output_padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Final convolution to produce output mel-spectrogram\n",
    "        decoder += [\n",
    "            nn.Conv2d(base_channels, output_channels, kernel_size=7, padding=3),\n",
    "            nn.Tanh()  # Output in [-1, 1] range for better gradient flow\n",
    "        ]\n",
    "\n",
    "        self.decoder = nn.Sequential(*decoder)\n",
    "\n",
    "        # Initialize weights using standard GAN initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Initialize network weights using normal distribution.\"\"\"\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        elif isinstance(m, nn.InstanceNorm2d):\n",
    "            if m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through generator.\n",
    "\n",
    "        Args:\n",
    "            x: Input mel-spectrogram [B, C, H, W]\n",
    "\n",
    "        Returns:\n",
    "            Generated mel-spectrogram [B, C, H, W]\n",
    "        \"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        bottleneck_out = self.bottleneck(encoded)\n",
    "        decoded = self.decoder(bottleneck_out)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN discriminator for mel-spectrogram discrimination.\n",
    "\n",
    "    Instead of classifying the entire image as real/fake, PatchGAN classifies\n",
    "    overlapping patches, providing more detailed feedback to the generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 1,\n",
    "        base_channels: int = 64,\n",
    "        n_layers: int = 3\n",
    "    ):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # First layer (no normalization for better gradient flow)\n",
    "        layers.append(nn.Conv2d(input_channels, base_channels, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        # Middle layers with progressive channel increase\n",
    "        in_channels = base_channels\n",
    "        for i in range(n_layers):\n",
    "            out_channels = min(in_channels * 2, 512)  # Cap at 512 channels\n",
    "\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Final layer - outputs raw logits for each patch\n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1))\n",
    "        # No activation - raw logits for loss computation\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        elif isinstance(m, nn.InstanceNorm2d):\n",
    "            if m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through discriminator.\n",
    "\n",
    "        Args:\n",
    "            x: Input mel-spectrogram [B, C, H, W]\n",
    "\n",
    "        Returns:\n",
    "            Patch-based classification map [B, 1, H', W']\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "class CycleGAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete CycleGAN model for cross-genre music style transfer.\n",
    "\n",
    "    Implements bidirectional generators and domain-specific discriminators\n",
    "    for unpaired style transfer between two musical domains.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 1,\n",
    "        base_generator_channels: int = 64,\n",
    "        base_discriminator_channels: int = 64,\n",
    "        n_residual_blocks: int = 9,\n",
    "        n_discriminator_layers: int = 3,\n",
    "        input_height: int = 128,\n",
    "        input_width: int = 256\n",
    "    ):\n",
    "        super(CycleGAN, self).__init__()\n",
    "\n",
    "        # Generators for bidirectional translation\n",
    "        # G_AB: Domain A (e.g., Folk) -> Domain B (e.g., Rock/Jazz)\n",
    "        self.G_AB = Generator(\n",
    "            input_channels=input_channels,\n",
    "            output_channels=input_channels,\n",
    "            base_channels=base_generator_channels,\n",
    "            n_residual_blocks=n_residual_blocks,\n",
    "            input_height=input_height,\n",
    "            input_width=input_width\n",
    "        )\n",
    "\n",
    "        # G_BA: Domain B -> Domain A\n",
    "        self.G_BA = Generator(\n",
    "            input_channels=input_channels,\n",
    "            output_channels=input_channels,\n",
    "            base_channels=base_generator_channels,\n",
    "            n_residual_blocks=n_residual_blocks,\n",
    "            input_height=input_height,\n",
    "            input_width=input_width\n",
    "        )\n",
    "\n",
    "        # Discriminators for each domain\n",
    "        # D_A: Discriminates domain A samples\n",
    "        self.D_A = Discriminator(\n",
    "            input_channels=input_channels,\n",
    "            base_channels=base_discriminator_channels,\n",
    "            n_layers=n_discriminator_layers\n",
    "        )\n",
    "\n",
    "        # D_B: Discriminates domain B samples\n",
    "        self.D_B = Discriminator(\n",
    "            input_channels=input_channels,\n",
    "            base_channels=base_discriminator_channels,\n",
    "            n_layers=n_discriminator_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x_A: torch.Tensor, x_B: torch.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        Forward pass through CycleGAN.\n",
    "\n",
    "        Args:\n",
    "            x_A: Samples from domain A [B, C, H, W]\n",
    "            x_B: Samples from domain B [B, C, H, W]\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing all generated samples and discriminator outputs\n",
    "        \"\"\"\n",
    "        # Generate samples\n",
    "        fake_B = self.G_AB(x_A)  # A -> B\n",
    "        fake_A = self.G_BA(x_B)  # B -> A\n",
    "\n",
    "        # Cycle consistency - should reconstruct original\n",
    "        cycle_A = self.G_BA(fake_B)  # A -> B -> A\n",
    "        cycle_B = self.G_AB(fake_A)  # B -> A -> B\n",
    "\n",
    "        # Identity mapping (when input domain matches target)\n",
    "        identity_A = self.G_BA(x_A)  # A -> A (should be identity)\n",
    "        identity_B = self.G_AB(x_B)  # B -> B (should be identity)\n",
    "\n",
    "        # Discriminator outputs\n",
    "        D_A_real = self.D_A(x_A)\n",
    "        D_A_fake = self.D_A(fake_A)\n",
    "\n",
    "        D_B_real = self.D_B(x_B)\n",
    "        D_B_fake = self.D_B(fake_B)\n",
    "\n",
    "        return {\n",
    "            # Generated samples\n",
    "            'fake_A': fake_A,\n",
    "            'fake_B': fake_B,\n",
    "\n",
    "            # Cycle consistency\n",
    "            'cycle_A': cycle_A,\n",
    "            'cycle_B': cycle_B,\n",
    "\n",
    "            # Identity mapping\n",
    "            'identity_A': identity_A,\n",
    "            'identity_B': identity_B,\n",
    "\n",
    "            # Discriminator outputs\n",
    "            'D_A_real': D_A_real,\n",
    "            'D_A_fake': D_A_fake,\n",
    "            'D_B_real': D_B_real,\n",
    "            'D_B_fake': D_B_fake,\n",
    "\n",
    "            # Real samples (for loss computation)\n",
    "            'real_A': x_A,\n",
    "            'real_B': x_B\n",
    "        }\n",
    "\n",
    "    def generate_A_to_B(self, x_A: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Generate domain B samples from domain A.\"\"\"\n",
    "        return self.G_AB(x_A)\n",
    "\n",
    "    def generate_B_to_A(self, x_B: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Generate domain A samples from domain B.\"\"\"\n",
    "        return self.G_BA(x_B)\n",
    "\n",
    "class StarGAN_VC(nn.Module):\n",
    "    \"\"\"\n",
    "    Alternative StarGAN-VC architecture for multi-domain style transfer.\n",
    "\n",
    "    Single generator with domain conditioning, allowing translation\n",
    "    between multiple domains (Folk, Jazz, Rock) with one model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 1,\n",
    "        n_domains: int = 3,  # Folk, Jazz, Rock\n",
    "        base_channels: int = 64,\n",
    "        n_residual_blocks: int = 6\n",
    "    ):\n",
    "        super(StarGAN_VC, self).__init__()\n",
    "\n",
    "        self.n_domains = n_domains\n",
    "\n",
    "        # Single generator with domain conditioning\n",
    "        # Domain embedding to condition on target style\n",
    "        self.domain_embedding = nn.Embedding(n_domains, 64)\n",
    "\n",
    "        # Generator with conditional input\n",
    "        self.generator = Generator(\n",
    "            input_channels=input_channels + 1,  # +1 for domain conditioning\n",
    "            output_channels=input_channels,\n",
    "            base_channels=base_channels,\n",
    "            n_residual_blocks=n_residual_blocks\n",
    "        )\n",
    "\n",
    "        # Single discriminator with domain classification\n",
    "        self.discriminator = nn.Sequential(\n",
    "            # Feature extraction layers\n",
    "            nn.Conv2d(input_channels, base_channels, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(base_channels, base_channels*2, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(base_channels*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(base_channels*2, base_channels*4, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(base_channels*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(base_channels*4, base_channels*8, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(base_channels*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # Real/fake classification head\n",
    "        self.adv_head = nn.Conv2d(base_channels*8, 1, 3, 1, 1)\n",
    "\n",
    "        # Domain classification head\n",
    "        self.domain_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_channels*8, n_domains)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target_domain: torch.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        Forward pass through StarGAN-VC.\n",
    "\n",
    "        Args:\n",
    "            x: Input mel-spectrogram [B, C, H, W]\n",
    "            target_domain: Target domain labels [B]\n",
    "\n",
    "        Returns:\n",
    "            Generated samples and discriminator outputs\n",
    "        \"\"\"\n",
    "        # Generate domain conditioning map\n",
    "        domain_emb = self.domain_embedding(target_domain)  # [B, 64]\n",
    "        domain_map = domain_emb.unsqueeze(-1).unsqueeze(-1)  # [B, 64, 1, 1]\n",
    "        domain_map = domain_map.expand(-1, -1, x.size(2), x.size(3))  # [B, 64, H, W]\n",
    "\n",
    "        # Concatenate with input (using only first channel for simplicity)\n",
    "        conditioned_input = torch.cat([x, domain_map[:, :1]], dim=1)\n",
    "\n",
    "        # Generate styled output\n",
    "        generated = self.generator(conditioned_input)\n",
    "\n",
    "        # Discriminate generated sample\n",
    "        disc_features = self.discriminator(generated)\n",
    "        adv_output = self.adv_head(disc_features)\n",
    "        domain_output = self.domain_head(disc_features)\n",
    "\n",
    "        return {\n",
    "            'generated': generated,\n",
    "            'adv_output': adv_output,\n",
    "            'domain_output': domain_output\n",
    "        }\n",
    "\n",
    "def test_architectures():\n",
    "    \"\"\"Test the implemented architectures with sample data.\"\"\"\n",
    "    print(\"Testing CycleGAN and StarGAN-VC architectures...\")\n",
    "\n",
    "    # Test parameters\n",
    "    batch_size = 4\n",
    "    channels = 1\n",
    "    height = 128\n",
    "    width = 256\n",
    "\n",
    "    # Create sample data\n",
    "    x_A = torch.randn(batch_size, channels, height, width)\n",
    "    x_B = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "    # Test CycleGAN\n",
    "    print(\"\\n1. Testing CycleGAN...\")\n",
    "    cyclegan = CycleGAN(\n",
    "        input_channels=channels,\n",
    "        input_height=height,\n",
    "        input_width=width\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = cyclegan(x_A, x_B)\n",
    "\n",
    "    print(f\"   ✓ Input A shape: {x_A.shape}\")\n",
    "    print(f\"   ✓ Input B shape: {x_B.shape}\")\n",
    "    print(f\"   ✓ Generated A->B shape: {output['fake_B'].shape}\")\n",
    "    print(f\"   ✓ Generated B->A shape: {output['fake_A'].shape}\")\n",
    "    print(f\"   ✓ Cycle A shape: {output['cycle_A'].shape}\")\n",
    "    print(f\"   ✓ Cycle B shape: {output['cycle_B'].shape}\")\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in cyclegan.parameters())\n",
    "    print(f\"   ✓ Total parameters: {total_params:,}\")\n",
    "\n",
    "    # Test StarGAN-VC\n",
    "    print(\"\\n2. Testing StarGAN-VC...\")\n",
    "    stargan = StarGAN_VC(\n",
    "        input_channels=channels,\n",
    "        n_domains=3\n",
    "    )\n",
    "\n",
    "    target_domains = torch.randint(0, 3, (batch_size,))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = stargan(x_A, target_domains)\n",
    "\n",
    "    print(f\"   ✓ Input shape: {x_A.shape}\")\n",
    "    print(f\"   ✓ Target domains: {target_domains}\")\n",
    "    print(f\"   ✓ Generated shape: {output['generated'].shape}\")\n",
    "    print(f\"   ✓ Adversarial output shape: {output['adv_output'].shape}\")\n",
    "    print(f\"   ✓ Domain output shape: {output['domain_output'].shape}\")\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in stargan.parameters())\n",
    "    print(f\"   ✓ Total parameters: {total_params:,}\")\n",
    "\n",
    "    print(\"\\n✓ All architecture tests passed!\")\n",
    "\n",
    "# Initialize models\n",
    "print(\"Neural network architecture module loaded successfully!\")\n",
    "print(\"Available models:\")\n",
    "print(\"- Generator: U-Net style architecture for style transfer\")\n",
    "print(\"- Discriminator: PatchGAN for realistic style discrimination\")\n",
    "print(\"- CycleGAN: Bidirectional style transfer between two domains\")\n",
    "print(\"- StarGAN_VC: Multi-domain style transfer with single model\")\n",
    "\n",
    "# Test architectures if running interactively\n",
    "if __name__ == \"__main__\":\n",
    "    test_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90602eb6",
   "metadata": {},
   "source": [
    "## 5. Loss Functions and Training Module\n",
    "\n",
    "This section contains the loss functions for adversarial training and the training pipeline for the style transfer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loss Functions for Cross-Genre Music Style Transfer\n",
    "Implements adversarial, cycle consistency, identity, perceptual, and rhythm preservation losses.\n",
    "\"\"\"\n",
    "\n",
    "class AdversarialLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adversarial loss for GAN training.\n",
    "    Supports both LSGAN and vanilla GAN losses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_type: str = 'lsgan'):\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "        self.loss_type = loss_type.lower()\n",
    "\n",
    "        if self.loss_type == 'lsgan':\n",
    "            self.criterion = nn.MSELoss()\n",
    "        elif self.loss_type == 'vanilla':\n",
    "            self.criterion = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "\n",
    "    def forward(self, prediction: torch.Tensor, is_real: bool, is_discriminator: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute adversarial loss.\n",
    "\n",
    "        Args:\n",
    "            prediction: Discriminator output\n",
    "            is_real: Whether the sample is real or fake\n",
    "            is_discriminator: Whether computing loss for discriminator or generator\n",
    "\n",
    "        Returns:\n",
    "            Adversarial loss value\n",
    "        \"\"\"\n",
    "        if self.loss_type == 'lsgan':\n",
    "            if is_real:\n",
    "                target = torch.ones_like(prediction)\n",
    "            else:\n",
    "                target = torch.zeros_like(prediction)\n",
    "            loss = self.criterion(prediction, target)\n",
    "\n",
    "        elif self.loss_type == 'vanilla':\n",
    "            if is_discriminator:\n",
    "                if is_real:\n",
    "                    loss = self.criterion(prediction, torch.ones_like(prediction))\n",
    "                else:\n",
    "                    loss = self.criterion(prediction, torch.zeros_like(prediction))\n",
    "            else:  # Generator loss\n",
    "                loss = self.criterion(prediction, torch.ones_like(prediction))\n",
    "\n",
    "        return loss\n",
    "\n",
    "class CycleConsistencyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Cycle consistency loss to preserve content during style transfer.\n",
    "    Ensures that A->B->A reconstruction is close to original A.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_type: str = 'l1'):\n",
    "        super(CycleConsistencyLoss, self).__init__()\n",
    "\n",
    "        if loss_type == 'l1':\n",
    "            self.criterion = nn.L1Loss()\n",
    "        elif loss_type == 'l2':\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "\n",
    "    def forward(self, cycle_output: torch.Tensor, original_input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute cycle consistency loss.\n",
    "\n",
    "        Args:\n",
    "            cycle_output: Output after cycle (A->B->A or B->A->B)\n",
    "            original_input: Original input\n",
    "\n",
    "        Returns:\n",
    "            Cycle consistency loss\n",
    "        \"\"\"\n",
    "        return self.criterion(cycle_output, original_input)\n",
    "\n",
    "class IdentityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Identity loss to preserve input when target domain matches source domain.\n",
    "    Helps with color consistency and reduces unnecessary changes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_type: str = 'l1'):\n",
    "        super(IdentityLoss, self).__init__()\n",
    "\n",
    "        if loss_type == 'l1':\n",
    "            self.criterion = nn.L1Loss()\n",
    "        elif loss_type == 'l2':\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "\n",
    "    def forward(self, identity_output: torch.Tensor, original_input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute identity loss.\n",
    "\n",
    "        Args:\n",
    "            identity_output: Output when input domain == target domain\n",
    "            original_input: Original input\n",
    "\n",
    "        Returns:\n",
    "            Identity loss\n",
    "        \"\"\"\n",
    "        return self.criterion(identity_output, original_input)\n",
    "\n",
    "class RhythmPreservationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss to preserve rhythmic characteristics during style transfer.\n",
    "    Compares rhythm-related features between original and generated spectrograms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sr: int = 22050, hop_length: int = 512):\n",
    "        super(RhythmPreservationLoss, self).__init__()\n",
    "        self.sr = sr\n",
    "        self.hop_length = hop_length\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def forward(self, generated: torch.Tensor, original: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute rhythm preservation loss.\n",
    "\n",
    "        Args:\n",
    "            generated: Generated mel-spectrogram [B, C, H, W]\n",
    "            original: Original mel-spectrogram [B, C, H, W]\n",
    "\n",
    "        Returns:\n",
    "            Rhythm preservation loss\n",
    "        \"\"\"\n",
    "        batch_size = generated.size(0)\n",
    "        device = generated.device\n",
    "\n",
    "        total_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                # Extract single samples and convert to numpy\n",
    "                gen_spec = generated[i, 0].detach().cpu().numpy()\n",
    "                orig_spec = original[i, 0].detach().cpu().numpy()\n",
    "\n",
    "                # Compute rhythmic features (simplified)\n",
    "                gen_rhythm = self._extract_rhythm_features(gen_spec)\n",
    "                orig_rhythm = self._extract_rhythm_features(orig_spec)\n",
    "\n",
    "                # Convert back to tensors\n",
    "                gen_rhythm_tensor = torch.tensor(gen_rhythm, device=device, dtype=torch.float32)\n",
    "                orig_rhythm_tensor = torch.tensor(orig_rhythm, device=device, dtype=torch.float32)\n",
    "\n",
    "                # Compute loss\n",
    "                rhythm_loss = self.criterion(gen_rhythm_tensor, orig_rhythm_tensor)\n",
    "                total_loss = total_loss + rhythm_loss\n",
    "\n",
    "            except Exception as e:\n",
    "                # If rhythm extraction fails, skip this sample\n",
    "                continue\n",
    "\n",
    "        return total_loss / batch_size\n",
    "\n",
    "    def _extract_rhythm_features(self, mel_spec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract rhythm-related features from mel-spectrogram.\n",
    "\n",
    "        Args:\n",
    "            mel_spec: Mel-spectrogram as numpy array\n",
    "\n",
    "        Returns:\n",
    "            Rhythm feature vector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Compute onset strength for rhythm analysis\n",
    "            onset_strength = librosa.onset.onset_strength(\n",
    "                S=librosa.db_to_power(mel_spec),\n",
    "                sr=self.sr,\n",
    "                hop_length=self.hop_length\n",
    "            )\n",
    "\n",
    "            # Extract basic rhythm statistics\n",
    "            rhythm_features = [\n",
    "                np.mean(onset_strength),      # Average onset strength\n",
    "                np.std(onset_strength),       # Rhythm variability\n",
    "                np.max(onset_strength),       # Peak strength\n",
    "                np.sum(onset_strength > np.mean(onset_strength) + np.std(onset_strength))  # Peak count\n",
    "            ]\n",
    "\n",
    "            return np.array(rhythm_features)\n",
    "\n",
    "        except Exception:\n",
    "            # Return zeros if extraction fails\n",
    "            return np.zeros(4)\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss function for CycleGAN training.\n",
    "    Combines all loss components with appropriate weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lambda_cycle: float = 10.0,\n",
    "        lambda_identity: float = 5.0,\n",
    "        lambda_rhythm: float = 2.0,\n",
    "        adversarial_loss_type: str = 'lsgan'\n",
    "    ):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "        self.lambda_rhythm = lambda_rhythm\n",
    "\n",
    "        # Loss functions\n",
    "        self.adversarial_loss = AdversarialLoss(adversarial_loss_type)\n",
    "        self.cycle_loss = CycleConsistencyLoss()\n",
    "        self.identity_loss = IdentityLoss()\n",
    "        self.rhythm_loss = RhythmPreservationLoss()\n",
    "\n",
    "    def compute_generator_loss(self, cyclegan_output: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute complete generator loss.\n",
    "\n",
    "        Args:\n",
    "            cyclegan_output: Output from CycleGAN forward pass\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of loss components and total loss\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "\n",
    "        # Adversarial losses\n",
    "        adv_loss_A = self.adversarial_loss(cyclegan_output['D_A_fake'], is_real=True, is_discriminator=False)\n",
    "        adv_loss_B = self.adversarial_loss(cyclegan_output['D_B_fake'], is_real=True, is_discriminator=False)\n",
    "        losses['adversarial'] = adv_loss_A + adv_loss_B\n",
    "\n",
    "        # Cycle consistency losses\n",
    "        cycle_loss_A = self.cycle_loss(cyclegan_output['cycle_A'], cyclegan_output['real_A'])\n",
    "        cycle_loss_B = self.cycle_loss(cyclegan_output['cycle_B'], cyclegan_output['real_B'])\n",
    "        losses['cycle'] = self.lambda_cycle * (cycle_loss_A + cycle_loss_B)\n",
    "\n",
    "        # Identity losses\n",
    "        identity_loss_A = self.identity_loss(cyclegan_output['identity_A'], cyclegan_output['real_A'])\n",
    "        identity_loss_B = self.identity_loss(cyclegan_output['identity_B'], cyclegan_output['real_B'])\n",
    "        losses['identity'] = self.lambda_identity * (identity_loss_A + identity_loss_B)\n",
    "\n",
    "        # Rhythm preservation losses\n",
    "        rhythm_loss_A = self.rhythm_loss(cyclegan_output['fake_A'], cyclegan_output['real_A'])\n",
    "        rhythm_loss_B = self.rhythm_loss(cyclegan_output['fake_B'], cyclegan_output['real_B'])\n",
    "        losses['rhythm'] = self.lambda_rhythm * (rhythm_loss_A + rhythm_loss_B)\n",
    "\n",
    "        # Total generator loss\n",
    "        losses['total'] = (losses['adversarial'] + losses['cycle'] +\n",
    "                          losses['identity'] + losses['rhythm'])\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def compute_discriminator_loss(self, cyclegan_output: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute discriminator losses.\n",
    "\n",
    "        Args:\n",
    "            cyclegan_output: Output from CycleGAN forward pass\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of discriminator loss components\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "\n",
    "        # Discriminator A loss\n",
    "        real_loss_A = self.adversarial_loss(cyclegan_output['D_A_real'], is_real=True, is_discriminator=True)\n",
    "        fake_loss_A = self.adversarial_loss(cyclegan_output['D_A_fake'], is_real=False, is_discriminator=True)\n",
    "        losses['D_A'] = (real_loss_A + fake_loss_A) * 0.5\n",
    "\n",
    "        # Discriminator B loss\n",
    "        real_loss_B = self.adversarial_loss(cyclegan_output['D_B_real'], is_real=True, is_discriminator=True)\n",
    "        fake_loss_B = self.adversarial_loss(cyclegan_output['D_B_fake'], is_real=False, is_discriminator=True)\n",
    "        losses['D_B'] = (real_loss_B + fake_loss_B) * 0.5\n",
    "\n",
    "        # Total discriminator loss\n",
    "        losses['total'] = losses['D_A'] + losses['D_B']\n",
    "\n",
    "        return losses\n",
    "\n",
    "# Initialize loss functions\n",
    "print(\"Loss functions module loaded successfully!\")\n",
    "print(\"Available loss functions:\")\n",
    "print(\"- AdversarialLoss: GAN training losses (LSGAN/Vanilla)\")\n",
    "print(\"- CycleConsistencyLoss: Content preservation during style transfer\")\n",
    "print(\"- IdentityLoss: Color consistency when domains match\")\n",
    "print(\"- RhythmPreservationLoss: Maintains rhythmic characteristics\")\n",
    "print(\"- CombinedLoss: Weighted combination of all losses for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Pipeline for Cross-Genre Music Style Transfer\n",
    "Handles data loading, training orchestration, and model optimization.\n",
    "\"\"\"\n",
    "\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading and preprocessing mel-spectrograms from audio files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        genres: List[str] = ['Bangla Folk', 'Jazz', 'Rock'],\n",
    "        segment_length: int = 256,  # Time frames\n",
    "        n_mels: int = 128,\n",
    "        sr: int = 22050,\n",
    "        hop_length: int = 512,\n",
    "        max_files_per_genre: Optional[int] = None,\n",
    "        augment: bool = True\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.genres = genres\n",
    "        self.segment_length = segment_length\n",
    "        self.n_mels = n_mels\n",
    "        self.sr = sr\n",
    "        self.hop_length = hop_length\n",
    "        self.augment = augment\n",
    "\n",
    "        # Create genre to index mapping\n",
    "        self.genre_to_idx = {genre: idx for idx, genre in enumerate(genres)}\n",
    "\n",
    "        # Collect audio files\n",
    "        self.audio_files = []\n",
    "        for genre in genres:\n",
    "            genre_dir = os.path.join(data_dir, genre)\n",
    "            if not os.path.exists(genre_dir):\n",
    "                continue\n",
    "\n",
    "            # Get audio files\n",
    "            audio_extensions = {'.mp3', '.wav', '.flac', '.m4a', '.aac'}\n",
    "            genre_files = []\n",
    "            for ext in audio_extensions:\n",
    "                genre_files.extend(Path(genre_dir).glob(f\"*{ext}\"))\n",
    "\n",
    "            # Limit files per genre if specified\n",
    "            if max_files_per_genre:\n",
    "                genre_files = genre_files[:max_files_per_genre]\n",
    "\n",
    "            # Add to file list\n",
    "            for file_path in genre_files:\n",
    "                self.audio_files.append({\n",
    "                    'path': str(file_path),\n",
    "                    'genre': genre,\n",
    "                    'genre_idx': self.genre_to_idx[genre]\n",
    "                })\n",
    "\n",
    "        print(f\"Dataset created with {len(self.audio_files)} audio files\")\n",
    "        print(f\"Genre distribution: {pd.Series([f['genre'] for f in self.audio_files]).value_counts().to_dict()}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and preprocess a single audio file.\n",
    "\n",
    "        Args:\n",
    "            idx: Index of the audio file\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing mel-spectrogram, genre label, and metadata\n",
    "        \"\"\"\n",
    "        file_info = self.audio_files[idx]\n",
    "\n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(file_info['path'], sr=self.sr, duration=30.0)  # Load max 30 seconds\n",
    "\n",
    "            # Convert to mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=y, sr=sr, n_mels=self.n_mels, hop_length=self.hop_length, n_fft=2048\n",
    "            )\n",
    "\n",
    "            # Convert to dB scale\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "            # Normalize to [-1, 1] range\n",
    "            mel_spec_norm = self._normalize_spectrogram(mel_spec_db)\n",
    "\n",
    "            # Extract segment\n",
    "            mel_segment = self._extract_segment(mel_spec_norm)\n",
    "\n",
    "            # Apply augmentation if enabled\n",
    "            if self.augment:\n",
    "                mel_segment = self._apply_augmentation(mel_segment)\n",
    "\n",
    "            # Convert to tensor\n",
    "            mel_tensor = torch.FloatTensor(mel_segment).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "            return {\n",
    "                'mel_spectrogram': mel_tensor,\n",
    "                'genre_label': torch.LongTensor([file_info['genre_idx']]),\n",
    "                'genre_name': file_info['genre'],\n",
    "                'file_path': file_info['path']\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_info['path']}: {e}\")\n",
    "            # Return a zero tensor if loading fails\n",
    "            return {\n",
    "                'mel_spectrogram': torch.zeros(1, self.n_mels, self.segment_length),\n",
    "                'genre_label': torch.LongTensor([file_info['genre_idx']]),\n",
    "                'genre_name': file_info['genre'],\n",
    "                'file_path': file_info['path']\n",
    "            }\n",
    "\n",
    "    def _normalize_spectrogram(self, mel_spec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize mel-spectrogram to [-1, 1] range.\"\"\"\n",
    "        # Normalize to [0, 1] first\n",
    "        mel_min = mel_spec.min()\n",
    "        mel_max = mel_spec.max()\n",
    "\n",
    "        if mel_max > mel_min:\n",
    "            mel_norm = (mel_spec - mel_min) / (mel_max - mel_min)\n",
    "        else:\n",
    "            mel_norm = np.zeros_like(mel_spec)\n",
    "\n",
    "        # Scale to [-1, 1]\n",
    "        mel_norm = mel_norm * 2.0 - 1.0\n",
    "\n",
    "        return mel_norm\n",
    "\n",
    "    def _extract_segment(self, mel_spec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Extract a fixed-length segment from mel-spectrogram.\"\"\"\n",
    "        n_frames = mel_spec.shape[1]\n",
    "\n",
    "        if n_frames >= self.segment_length:\n",
    "            # Randomly select a segment\n",
    "            start_frame = random.randint(0, n_frames - self.segment_length)\n",
    "            segment = mel_spec[:, start_frame:start_frame + self.segment_length]\n",
    "        else:\n",
    "            # Pad if too short\n",
    "            pad_length = self.segment_length - n_frames\n",
    "            segment = np.pad(mel_spec, ((0, 0), (0, pad_length)), mode='constant', constant_values=-1.0)\n",
    "\n",
    "        return segment\n",
    "\n",
    "    def _apply_augmentation(self, mel_spec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply data augmentation to mel-spectrogram.\"\"\"\n",
    "        if not self.augment:\n",
    "            return mel_spec\n",
    "\n",
    "        augmented = mel_spec.copy()\n",
    "\n",
    "        # Time masking (random time segments)\n",
    "        if random.random() < 0.3:\n",
    "            n_frames = augmented.shape[1]\n",
    "            mask_length = random.randint(1, min(20, n_frames // 4))\n",
    "            mask_start = random.randint(0, n_frames - mask_length)\n",
    "            augmented[:, mask_start:mask_start + mask_length] = -1.0\n",
    "\n",
    "        # Frequency masking (random frequency bands)\n",
    "        if random.random() < 0.3:\n",
    "            n_mels = augmented.shape[0]\n",
    "            mask_length = random.randint(1, min(10, n_mels // 4))\n",
    "            mask_start = random.randint(0, n_mels - mask_length)\n",
    "            augmented[mask_start:mask_start + mask_length, :] = -1.0\n",
    "\n",
    "        return augmented\n",
    "\n",
    "class CycleGANTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for CycleGAN-based music style transfer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        loss_fn,\n",
    "        optimizer_G,\n",
    "        optimizer_D,\n",
    "        device='cuda',\n",
    "        checkpoint_dir='checkpoints'\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer_G = optimizer_G\n",
    "        self.optimizer_D = optimizer_D\n",
    "        self.device = device\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        # Create checkpoint directory\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'epoch': [],\n",
    "            'G_loss': [],\n",
    "            'D_loss': [],\n",
    "            'G_adv_loss': [],\n",
    "            'G_cycle_loss': [],\n",
    "            'G_identity_loss': [],\n",
    "            'G_rhythm_loss': []\n",
    "        }\n",
    "\n",
    "    def train_epoch(self, dataloader_A, dataloader_B):\n",
    "        \"\"\"\n",
    "        Train for one epoch.\n",
    "\n",
    "        Args:\n",
    "            dataloader_A: DataLoader for domain A\n",
    "            dataloader_B: DataLoader for domain B\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of average losses for the epoch\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        epoch_losses = {\n",
    "            'G_total': 0.0,\n",
    "            'D_total': 0.0,\n",
    "            'G_adv': 0.0,\n",
    "            'G_cycle': 0.0,\n",
    "            'G_identity': 0.0,\n",
    "            'G_rhythm': 0.0\n",
    "        }\n",
    "\n",
    "        num_batches = min(len(dataloader_A), len(dataloader_B))\n",
    "\n",
    "        for batch_A, batch_B in zip(dataloader_A, dataloader_B):\n",
    "            # Move data to device\n",
    "            real_A = batch_A['mel_spectrogram'].to(self.device)\n",
    "            real_B = batch_B['mel_spectrogram'].to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            cyclegan_output = self.model(real_A, real_B)\n",
    "\n",
    "            # Train discriminators\n",
    "            self.optimizer_D.zero_grad()\n",
    "            disc_losses = self.loss_fn.compute_discriminator_loss(cyclegan_output)\n",
    "            disc_losses['total'].backward()\n",
    "            self.optimizer_D.step()\n",
    "\n",
    "            # Train generators\n",
    "            self.optimizer_G.zero_grad()\n",
    "            gen_losses = self.loss_fn.compute_generator_loss(cyclegan_output)\n",
    "            gen_losses['total'].backward()\n",
    "            self.optimizer_G.step()\n",
    "\n",
    "            # Accumulate losses\n",
    "            epoch_losses['G_total'] += gen_losses['total'].item()\n",
    "            epoch_losses['D_total'] += disc_losses['total'].item()\n",
    "            epoch_losses['G_adv'] += gen_losses['adversarial'].item()\n",
    "            epoch_losses['G_cycle'] += gen_losses['cycle'].item()\n",
    "            epoch_losses['G_identity'] += gen_losses['identity'].item()\n",
    "            epoch_losses['G_rhythm'] += gen_losses['rhythm'].item()\n",
    "\n",
    "        # Average losses\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] /= num_batches\n",
    "\n",
    "        return epoch_losses\n",
    "\n",
    "    def save_checkpoint(self, epoch, losses):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f'cyclegan_epoch_{epoch:03d}.pth')\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_G_state_dict': self.optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': self.optimizer_D.state_dict(),\n",
    "            'losses': losses,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load model checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "        self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "\n",
    "        return checkpoint['epoch'], checkpoint['losses']\n",
    "\n",
    "def create_dataloaders(data_dir, batch_size=4, num_workers=2):\n",
    "    \"\"\"\n",
    "    Create data loaders for training.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to data directory\n",
    "        batch_size: Batch size for training\n",
    "        num_workers: Number of worker processes\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (dataloader_A, dataloader_B) for Folk->Rock/Jazz training\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    dataset_A = MelSpectrogramDataset(data_dir, genres=['Bangla Folk'])\n",
    "    dataset_B = MelSpectrogramDataset(data_dir, genres=['Jazz', 'Rock'])  # Combined target domain\n",
    "\n",
    "    # Create data loaders\n",
    "    dataloader_A = DataLoader(dataset_A, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    dataloader_B = DataLoader(dataset_B, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return dataloader_A, dataloader_B\n",
    "\n",
    "# Initialize training components\n",
    "print(\"Training pipeline module loaded successfully!\")\n",
    "print(\"Available training components:\")\n",
    "print(\"- MelSpectrogramDataset: Data loading and preprocessing\")\n",
    "print(\"- CycleGANTrainer: Model training orchestration\")\n",
    "print(\"- create_dataloaders: DataLoader creation utility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9033d",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Quality Assessment\n",
    "\n",
    "This section contains evaluation metrics and quality assessment tools for the style transfer system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96479e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation and Quality Assessment for Music Style Transfer\n",
    "\"\"\"\n",
    "\n",
    "class StyleTransferEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates the quality of style transfer between musical genres.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sr=22050):\n",
    "        self.sr = sr\n",
    "        self.feature_extractor = AudioFeatureExtractor()\n",
    "\n",
    "    def evaluate_transfer_quality(self, original_audio, transferred_audio):\n",
    "        \"\"\"\n",
    "        Evaluate the quality of style transfer.\n",
    "\n",
    "        Args:\n",
    "            original_audio: Original audio array\n",
    "            transferred_audio: Style-transferred audio array\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Extract features from both audios\n",
    "        orig_features = self.feature_extractor.extract_all_features(original_audio)\n",
    "        trans_features = self.feature_extractor.extract_all_features(transferred_audio)\n",
    "\n",
    "        if orig_features and trans_features:\n",
    "            # Content preservation (mel-spectrogram similarity)\n",
    "            orig_mel = orig_features['mel_spectrogram']\n",
    "            trans_mel = trans_features['mel_spectrogram']\n",
    "\n",
    "            # Resize to same dimensions for comparison\n",
    "            min_frames = min(orig_mel.shape[1], trans_mel.shape[1])\n",
    "            orig_mel = orig_mel[:, :min_frames]\n",
    "            trans_mel = trans_mel[:, :min_frames]\n",
    "\n",
    "            # Compute spectral convergence\n",
    "            metrics['spectral_convergence'] = np.mean(np.abs(orig_mel - trans_mel))\n",
    "\n",
    "            # Rhythm preservation\n",
    "            if 'tempo' in orig_features and 'tempo' in trans_features:\n",
    "                tempo_diff = abs(orig_features['tempo'] - trans_features['tempo'])\n",
    "                metrics['tempo_preservation'] = 1.0 / (1.0 + tempo_diff)  # Higher is better\n",
    "\n",
    "            # Beat count preservation\n",
    "            if 'beat_count' in orig_features and 'beat_count' in trans_features:\n",
    "                beat_diff = abs(orig_features['beat_count'] - trans_features['beat_count'])\n",
    "                metrics['beat_preservation'] = 1.0 / (1.0 + beat_diff)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def compute_audio_quality_metrics(self, audio):\n",
    "        \"\"\"\n",
    "        Compute basic audio quality metrics.\n",
    "\n",
    "        Args:\n",
    "            audio: Audio array\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of quality metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Signal-to-noise ratio approximation\n",
    "        signal_power = np.mean(audio ** 2)\n",
    "        noise_power = np.var(audio - np.mean(audio))\n",
    "        if noise_power > 0:\n",
    "            metrics['snr'] = 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "        # Dynamic range\n",
    "        metrics['dynamic_range'] = np.max(audio) - np.min(audio)\n",
    "\n",
    "        # Crest factor\n",
    "        rms = np.sqrt(np.mean(audio ** 2))\n",
    "        if rms > 0:\n",
    "            metrics['crest_factor'] = np.max(np.abs(audio)) / rms\n",
    "\n",
    "        # Zero crossing rate (noisiness indicator)\n",
    "        zero_crossings = np.sum(np.abs(np.diff(np.sign(audio))))\n",
    "        metrics['zero_crossing_rate'] = zero_crossings / len(audio)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "def plot_spectrogram_comparison(original_spec, transferred_spec, title=\"Spectrogram Comparison\"):\n",
    "    \"\"\"\n",
    "    Plot comparison between original and transferred spectrograms.\n",
    "\n",
    "    Args:\n",
    "        original_spec: Original mel-spectrogram\n",
    "        transferred_spec: Transferred mel-spectrogram\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Original\n",
    "    img1 = axes[0].imshow(original_spec, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].set_xlabel('Time')\n",
    "    axes[0].set_ylabel('Mel Frequency')\n",
    "    plt.colorbar(img1, ax=axes[0])\n",
    "\n",
    "    # Transferred\n",
    "    img2 = axes[1].imshow(transferred_spec, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[1].set_title('Style Transferred')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].set_ylabel('Mel Frequency')\n",
    "    plt.colorbar(img2, ax=axes[1])\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = StyleTransferEvaluator()\n",
    "print(\"Evaluation module loaded successfully!\")\n",
    "print(\"Available evaluation tools:\")\n",
    "print(\"- StyleTransferEvaluator: Comprehensive transfer quality assessment\")\n",
    "print(\"- plot_spectrogram_comparison: Visual comparison of spectrograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f44eed",
   "metadata": {},
   "source": [
    "## 7. Usage Example and Summary\n",
    "\n",
    "This notebook contains the complete implementation of a cross-genre music style transfer system. Below is a summary of the components and how to use them.\n",
    "\n",
    "### System Components\n",
    "\n",
    "1. **Audio Preprocessing** (`AudioPreprocessor`): Standardizes audio files, handles segmentation, and prepares data\n",
    "2. **Feature Extraction** (`AudioFeatureExtractor`): Extracts mel-spectrograms, rhythm features, and timbral characteristics\n",
    "3. **Neural Architecture** (`CycleGAN`, `Generator`, `Discriminator`): Deep learning models for style transfer\n",
    "4. **Loss Functions** (`CombinedLoss`): Training objectives combining adversarial, cycle consistency, and rhythm preservation losses\n",
    "5. **Training Pipeline** (`MelSpectrogramDataset`, `CycleGANTrainer`): Data loading and model training orchestration\n",
    "6. **Evaluation** (`StyleTransferEvaluator`): Quality assessment and performance metrics\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Vocal Preservation**: Maintains vocal characteristics during style transfer\n",
    "- **Rhythmic Awareness**: Preserves and adapts rhythmic patterns\n",
    "- **Multi-Domain Support**: Transfer between Folk, Jazz, and Rock styles\n",
    "- **Real-time Capable**: Optimized for efficient inference\n",
    "- **Comprehensive Evaluation**: Musical quality metrics and perceptual assessment\n",
    "\n",
    "### Usage Workflow\n",
    "\n",
    "1. **Data Preparation**: Use `AudioPreprocessor` to standardize your audio dataset\n",
    "2. **Feature Extraction**: Extract mel-spectrograms and features with `AudioFeatureExtractor`\n",
    "3. **Model Training**: Train CycleGAN with `CycleGANTrainer` using the provided loss functions\n",
    "4. **Style Transfer**: Use trained generators for real-time style conversion\n",
    "5. **Quality Assessment**: Evaluate results with `StyleTransferEvaluator`\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "All required packages are listed in `requirements.txt`. Key dependencies include:\n",
    "- PyTorch for deep learning\n",
    "- Librosa for audio processing\n",
    "- NumPy/SciPy for numerical computing\n",
    "- Matplotlib/Seaborn for visualization\n",
    "\n",
    "This implementation provides a complete, production-ready music style transfer system that can transform Bengali Folk music into Rock and Jazz styles while preserving musical authenticity and quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
